python3 train.py --train_new \
  --train_dir ../gtaai_datasets/train \
  --val_dir  ../gtaai_datasets/dev \
  --output_dir models/tedd_1104_large_adamw \
  --encoder_type transformer \
  --dataloader_num_workers 20 \
  --batch_size 8 \
  --accumulation_steps 2 \
  --max_epochs 20 \
  --cnn_model_name efficientnet_v2_l \
  --num_layers_encoder 6 \
  --embedded_size 512 \
  --learning_rate 3e-5 \
  --optimizer_name adamw \
  --scheduler_name linear \
  --warmup_factor 0.05 \
  --mask_prob 0.2 \
  --hide_map_prob 0.0 \
  --dropout_cnn_out 0.3 \
  --dropout_encoder 0.1 \
  --dropout_encoder_features 0.5 \
  --label_smoothing 0.1 \
  --control_mode keyboard \
  --val_check_interval 0.25 \
  --precision "16"  \
  --devices 2 \
  --strategy "ddp_find_unused_parameters_false"


python3 train.py --train_new \
  --train_dir ../gtaai_datasets/train \
  --val_dir  ../gtaai_datasets/dev \
  --output_dir models/tedd_1104_large_adafactor_1e-3 \
  --encoder_type transformer \
  --dataloader_num_workers 20 \
  --batch_size 8 \
  --accumulation_steps 1 \
  --max_epochs 20 \
  --cnn_model_name efficientnet_v2_l \
  --num_layers_encoder 6 \
  --embedded_size 512 \
  --learning_rate 1e-3 \
  --optimizer_name adafactor \
  --scheduler_name linear \
  --warmup_factor 0.05 \
  --mask_prob 0.2 \
  --hide_map_prob 0.0 \
  --dropout_cnn_out 0.3 \
  --dropout_encoder 0.1 \
  --dropout_encoder_features 0.5 \
  --label_smoothing 0.1 \
  --control_mode keyboard \
  --val_check_interval 0.25 \
  --precision "16"  \
  --devices 4 \
  --strategy "ddp_find_unused_parameters_false"